---
title: "LLMs - CrewAI"
source: "https://docs.crewai.com/concepts/llms"
author:
  - "[[CrewAI]]"
published:
created: 2025-01-17
description: "A comprehensive guide to configuring and using Large Language Models (LLMs) in your CrewAI projects"
tags:
  - "clippings"
---

Core Concepts

# LLMs

A comprehensive guide to configuring and using Large Language Models (LLMs) in your CrewAI projects

CrewAI integrates with multiple LLM providers through LiteLLM, giving you the flexibility to choose the right model for your specific use case. This guide will help you understand how to configure and use different LLM providers in your CrewAI projects.

## [What are LLMs?​](https://docs.crewai.com/concepts/#what-are-llms)

Large Language Models (LLMs) are the core intelligence behind CrewAI agents. They enable agents to understand context, make decisions, and generate human-like responses. Here’s what you need to know:

## LLM Basics

Large Language Models are AI systems trained on vast amounts of text data. They power the intelligence of your CrewAI agents, enabling them to understand and generate human-like text.

## Context Window

The context window determines how much text an LLM can process at once. Larger windows (e.g., 128K tokens) allow for more context but may be more expensive and slower.

## Temperature

Temperature (0.0 to 1.0) controls response randomness. Lower values (e.g., 0.2) produce more focused, deterministic outputs, while higher values (e.g., 0.8) increase creativity and variability.

## Provider Selection

Each LLM provider (e.g., OpenAI, Anthropic, Google) offers different models with varying capabilities, pricing, and features. Choose based on your needs for accuracy, speed, and cost.

## [Available Models and Their Capabilities​](https://docs.crewai.com/concepts/#available-models-and-their-capabilities)

Here’s a detailed breakdown of supported models and their capabilities, you can compare performance at [lmarena.ai](https://lmarena.ai/?leaderboard) and [artificialanalysis.ai](https://artificialanalysis.ai/):

| Model                | Context Window | Best For                                |
| -------------------- | -------------- | --------------------------------------- |
| GPT-4                | 8,192 tokens   | High-accuracy tasks, complex reasoning  |
| GPT-4 Turbo          | 128,000 tokens | Long-form content, document analysis    |
| GPT-4o & GPT-4o-mini | 128,000 tokens | Cost-effective large context processing |

1 token ≈ 4 characters in English. For example, 8,192 tokens ≈ 32,768 characters or about 6,000 words.

## [Setting Up Your LLM​](https://docs.crewai.com/concepts/#setting-up-your-llm)

There are three ways to configure LLMs in CrewAI. Choose the method that best fits your workflow:
The simplest way to get started. Set these variables in your environment:

```bash
# Required: Your API key for authentication
OPENAI_API_KEY=<your-api-key>

# Optional: Default model selection
OPENAI_MODEL_NAME=gpt-4o-mini  # Default if not set

# Optional: Organization ID (if applicable)
OPENAI_ORGANIZATION_ID=<your-org-id>
```

Never commit API keys to version control. Use environment files (.env) or your system’s secret management.

## [Advanced Features and Optimization​](https://docs.crewai.com/concepts/#advanced-features-and-optimization)

Learn how to get the most out of your LLM configuration:

CrewAI includes smart context management features:

```python
from crewai import LLM

# CrewAI automatically handles:
# 1. Token counting and tracking
# 2. Content summarization when needed
# 3. Task splitting for large contexts

llm = LLM(
    model="gpt-4",
    max_tokens=4000,  # Limit response length
)
```

Best practices for context management:

1. Choose models with appropriate context windows
2. Pre-process long inputs when possible
3. Use chunking for large documents
4. Monitor token usage to optimize costs

1 Token Usage Optimization

Choose the right context window for your task:

- Small tasks (up to 4K tokens): Standard models
- Medium tasks (between 4K-32K): Enhanced models
- Large tasks (over 32K): Large context models

```python
# Configure model with appropriate settings
llm = LLM(
    model="openai/gpt-4-turbo-preview",
    temperature=0.7,    # Adjust based on task
    max_tokens=4096,    # Set based on output needs
    timeout=300        # Longer timeout for complex tasks
)
```

- Lower temperature (0.1 to 0.3) for factual responses
- Higher temperature (0.7 to 0.9) for creative tasks

2 Best Practices

1. Monitor token usage
2. Implement rate limiting
3. Use caching when possible
4. Set appropriate max_tokens limits

Remember to regularly monitor your token usage and adjust your configuration as needed to optimize costs and performance.

## [Provider Configuration Examples​](https://docs.crewai.com/concepts/#provider-configuration-examples)

```python
# Required
OPENAI_API_KEY=sk-...

# Optional
OPENAI_API_BASE=<custom-base-url>
OPENAI_ORGANIZATION=<your-org-id>
```

Example usage:

```python
from crewai import LLM

llm = LLM(
    model="gpt-4",
    temperature=0.8,
    max_tokens=150,
    top_p=0.9,
    frequency_penalty=0.1,
    presence_penalty=0.1,
    stop=["END"],
    seed=42
)
```

```python
ANTHROPIC_API_KEY=sk-ant-...
```

Example usage:

```python
llm = LLM(
    model="anthropic/claude-3-sonnet-20240229-v1:0",
    temperature=0.7
)
```

```python
# Option 1. Gemini accessed with an API key.
# https://ai.google.dev/gemini-api/docs/api-key
GEMINI_API_KEY=<your-api-key>

# Option 2. Vertex AI IAM credentials for Gemini, Anthropic, and anything in the Model Garden.
# https://cloud.google.com/vertex-ai/generative-ai/docs/overview
```

Example usage:

```python
llm = LLM(
    model="gemini/gemini-1.5-pro-latest",
    temperature=0.7
)
```

```python
# Required
AZURE_API_KEY=<your-api-key>
AZURE_API_BASE=<your-resource-url>
AZURE_API_VERSION=<api-version>

# Optional
AZURE_AD_TOKEN=<your-azure-ad-token>
AZURE_API_TYPE=<your-azure-api-type>
```

Example usage:

```python
llm = LLM(
    model="azure/gpt-4",
    api_version="2023-05-15"
)
```

```python
AWS_ACCESS_KEY_ID=<your-access-key>
AWS_SECRET_ACCESS_KEY=<your-secret-key>
AWS_DEFAULT_REGION=<your-region>
```

Example usage:

```python
llm = LLM(
    model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0"
)
```

```python
MISTRAL_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="mistral/mistral-large-latest",
    temperature=0.7
)
```

```python
NVIDIA_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="nvidia_nim/meta/llama3-70b-instruct",
    temperature=0.7
)
```

```python
GROQ_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="groq/llama-3.2-90b-text-preview",
    temperature=0.7
)
```

```python
# Required
WATSONX_URL=<your-url>
WATSONX_APIKEY=<your-apikey>
WATSONX_PROJECT_ID=<your-project-id>

# Optional
WATSONX_TOKEN=<your-token>
WATSONX_DEPLOYMENT_SPACE_ID=<your-space-id>
```

Example usage:

```python
llm = LLM(
    model="watsonx/meta-llama/llama-3-1-70b-instruct",
    base_url="https://api.watsonx.ai/v1"
)
```

1. Install Ollama: [ollama.ai](https://ollama.ai/)
2. Run a model: `ollama run llama2`
3. Configure:

```python
llm = LLM(
    model="ollama/llama3:70b",
    base_url="http://localhost:11434"
)
```

```python
FIREWORKS_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct",
    temperature=0.7
)
```

```python
PERPLEXITY_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="llama-3.1-sonar-large-128k-online",
    base_url="https://api.perplexity.ai/"
)
```

```python
HUGGINGFACE_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct",
    base_url="your_api_endpoint"
)
```

```python
SAMBANOVA_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="sambanova/Meta-Llama-3.1-8B-Instruct",
    temperature=0.7
)
```

```python
# Required
CEREBRAS_API_KEY=<your-api-key>
```

Example usage:

```python
llm = LLM(
    model="cerebras/llama3.1-70b",
    temperature=0.7,
    max_tokens=8192
)
```

Cerebras features:

- Fast inference speeds
- Competitive pricing
- Good balance of speed and quality
- Support for long context windows

## [Common Issues and Solutions​](https://docs.crewai.com/concepts/#common-issues-and-solutions)

Most authentication issues can be resolved by checking API key format and environment variable names.

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...
```

## [Getting Help​](https://docs.crewai.com/concepts/#getting-help)

If you need assistance, these resources are available:

## LiteLLM Documentation

[Comprehensive documentation for LiteLLM integration and troubleshooting common issues.](https://docs.litellm.ai/docs/)[

## GitHub Issues

[Report bugs, request features, or browse existing issues for solutions.](https://github.com/joaomdmoura/crewAI/issues)[
